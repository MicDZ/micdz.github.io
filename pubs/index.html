<!DOCTYPE html>
<html lang="en,zh_cn,default">

<head>
  <meta charset="utf-8">
  
  <title>All Publications - MicDZ</title>

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/normalize.css">

  <style>
  /* stylelint-disable */
  a:not(.icon):not(.clustrmaps-map-control):not(.formula-anchor) {
    text-decoration-color: #000;
  }
  a:hover:not(.icon):not(.clustrmaps-map-control):not(.formula-anchor),
  a:focus:not(.icon):not(.clustrmaps-map-control):not(.formula-anchor) {
    text-decoration-color: #111;
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  /* stylelint-enable */
</style>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/png" href="/img/h.jpeg">
  <link rel="stylesheet" href="https://cdn.jsdmirror.com
/npm/lxgw-wenkai-webfont@1.1.0/style.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.1/moment.min.js"></script>
  <!-- KaTeX support -->
  <link rel="stylesheet" href="https://cdn.jsdmirror.com
/npm/katex@0.16.9/dist/katex.css" integrity="sha384-OH8qNTHoMMVNVcKdKewlipV4SErXqccxxlg6HC9Cwjr5oZu2AdBej1TndeCirael" crossorigin="anonymous">
<script defer src="https://cdn.jsdmirror.com
/npm/katex@0.16.9/dist/katex.js" integrity="sha384-ImWHyYvDPlRzwFnvLhG9Jcpm/7qL4SVIuJK0C6Rj+Mf7fpeNJCLCynC+EVz4kCSA" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"
        integrity="sha512-EKW5YvKU3hpyyOcN6jQnAxO/L8gts+YdYV6Yymtl8pk9YlYFtqJgihORuRoBXK8/cOIlappdU6Ms8KdK6yBCgA=="
        crossorigin="anonymous" referrerpolicy="no-referrer">
</script>
<link rel="stylesheet" href="https://cdn.jsdmirror.com
/npm/pseudocode@2.4.1/build/pseudocode.min.css">
<script src="https://cdn.jsdmirror.com
/npm/pseudocode@2.4.1/build/pseudocode.min.js">
</script>

<!-- Prism support ./js/prism.js-->
  
<link rel="stylesheet" href="/css/prism.css">

<meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="MicDZ's Blog" type="application/atom+xml">
</head>

<body>
    <div class="container">
        <div class="row">
            <div class="row">

    <div class="two columns" style="max-width: 50px">
      <h1 class="mt-2 mode">
        <div onclick=setDarkMode(true) id="darkBtn"></div>
        <div onclick=setDarkMode(false) id="lightBtn" class=hidden></div>
        <script >
          if (localStorage.getItem('preferredTheme') == 'dark') {
            setDarkMode(true)
          }
          function setDarkMode(isDark) {
            var darkBtn = document.getElementById('darkBtn')
            var lightBtn = document.getElementById('lightBtn')
            if (isDark) {
              lightBtn.style.display = "block"
              darkBtn.style.display = "none"
              localStorage.setItem('preferredTheme', 'dark');
            } else {
              lightBtn.style.display = "none"
              darkBtn.style.display = "block"
              localStorage.removeItem('preferredTheme');
            }
            document.body.classList.toggle("darkmode");
          }
        </script>
      </h1>
    </div>
     
    <div class="six columns ml-1">
      <h1 class="mt-2 site-title">
        MicDZ&#39;s Blog
      </h1>
    </div>
  
    <div class="twelve columns">
      <div class="row">
        <div class="right">
          
            <a href="/" class="ml">Home</a>
          
          
          
            <a href="/archives" class="ml">Archives</a>
             

          <!-- <a href="/" class="ml">About</a> -->
          
            <a href="/friends" class="ml">Friends</a>
           
          
          
        </div>
      </div>
      <hr style="margin-bottom: 2.6rem">
    </div>
    


  </div>
          <div>
            <!-- <div class="trans"> -->
              

 <div style="display: flex; justify-content: space-between; align-items: center;">
  <h2>Conference and Journal Publications</h2>
  <p>[<a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=Uz7F5LwAAAAJ&hl=zh-CN">Google Scholar</a>]</p>
 </div>


    
      <div class="mb-3">
        
        <div class="article-list spacing has-figure">
          <div class="publication-text">
            <p>
            
              <b>From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment</b>
              
              <!-- 显示所有的作者 -->
              
                <br>
              <span class="authors">
              <!-- 添加一个变量 -->

                
                  
                  
                    
                  
                  
                     
                  
                  <u>Han Zhou</u><sup>*</sup>,  
                
                  
                  
                    
                  
                  
                     
                  
                  Jinjin Cao<sup>*</sup>,  
                
                  
                  
                    
                  
                  
                     
                  
                  Liyuan Ma<sup>†</sup>,  
                
                  
                  
                    
                  
                  
                     
                  
                  Xueji Fang,  
                
                  
                  
                    
                  
                  
                  Guo-jun Qi 
                
            
            </span>
  
              
                <br>
              <p>
                Preprint ArXiv 2510.00491, 
  
              
              
                                Sep. 2025
              </p >
                
              <div>
                <p>
                  
                    [<a href="#" class="abstract-link" data-abstract="Learning diverse manipulation skills for real-world robots is severely bottlenecked by the reliance on costly and hard-to-scale teleoperated demonstrations. While human videos offer a scalable alternative, effectively transferring manipulation knowledge is fundamentally hindered by the significant morphological gap between human and robotic embodiments. To address this challenge and facilitate skill transfer from human to robot, we introduce Traj2Action,a novel framework that bridges this embodiment gap by using the 3D trajectory of the operational endpoint as a unified intermediate representation, and then transfers the manipulation knowledge embedded in this trajectory to the robot&#39;s actions. Our policy first learns to generate a coarse trajectory, which forms an high-level motion plan by leveraging both human and robot data. This plan then conditions the synthesis of precise, robot-specific actions (e.g., orientation and gripper state) within a co-denoising framework. Extensive real-world experiments on a Franka robot demonstrate that Traj2Action boosts the performance by up to 27% and 22.25% over π0 baseline on short- and long-horizon real-world tasks, and achieves significant gains as human data scales in robot policy learning. ">Abstract</a>]
                  
                  
                    [<a target="_blank" rel="noopener" href="https://github.com/MicDZ/Traj2Action">GitHub</a>]
                  
                  
                    [<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.00491">arXiv</a>]
                  
                  
                    [<a target="_blank" rel="noopener" href="https://micdz.github.io/Traj2Action/">Home Page</a>]
                  
                  
                    [<a target="_blank" rel="noopener" href="https://huggingface.co/Traj2Action">Hugging Face</a>]
                  

                </p>
              </div>
  
  
            </div>
              
              <div class="publication-figure">
                <div class="media-aspect">
                  <img src="https://micdz.github.io/Traj2Action/static/images/main.png" alt="publication">
                </div>
              </div>
              
           </div>
         
       </div>

      <div class="mb-3">
        
        <div class="article-list spacing has-figure">
          <div class="publication-text">
            <p>
            
              <b>MANBench: Is Your Multimodal Model Smarter than Human?</b>
              
              <!-- 显示所有的作者 -->
              
                <br>
              <span class="authors">
              <!-- 添加一个变量 -->

                
                  
                  
                    
                  
                  
                     
                  
                  <u>Han Zhou</u>,  
                
                  
                  
                    
                  
                  
                     
                  
                  Qitong Xu,  
                
                  
                  
                    
                  
                  
                     
                  
                  Yiheng Dong,  
                
                  
                  
                    
                  
                  
                  Xin Yang<sup>†<sup> 
                
            
            </span>
  
              
                <br>
              <p>
                ACL'25 Findings, 
  
              
              
                                Feb. 2025
              </p >
                
              <div>
                <p>
                  
                    [<a href="#" class="abstract-link" data-abstract="The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited discussions regarding their potential to surpass human performance in multimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms Benchmark), a bilingual benchmark (English and Chinese) comprising 1,314 questions across nine tasks, spanning knowledge-based and non-knowledge-based domains. MANBench emphasizes intuitive reasoning, seamless cross-modal integration, and real-world complexity, providing a rigorous evaluation framework. Through extensive human experiments involving diverse participants, we compared human performance against state-of-the-art MLLMs. The results indicate that while MLLMs excel in tasks like Knowledge and Text-Image Understanding, they struggle with deeper cross-modal reasoning tasks such as Transmorphic Understanding, Image Consistency, and Multi-image Understanding. Moreover, both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination. MANBench highlights the strengths and limitations of MLLMs, revealing that even advanced models fall short of achieving human-level performance across many domains. We hope MANBench will inspire efforts to bridge the gap between MLLMs and human multimodal capabilities.">Abstract</a>]
                  
                  
                    [<a target="_blank" rel="noopener" href="https://github.com/micdz/MANBench">GitHub</a>]
                  
                  
                    [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11080">arXiv</a>]
                  
                  
                    [<a href="/MANBench/">Home Page</a>]
                  
                  
                    [<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/MANBench/MANBench">Hugging Face</a>]
                  

                </p>
              </div>
  
  
            </div>
              
              <div class="publication-figure">
                <div class="media-aspect">
                  <img src="/MANBench/static/images/teaser.jpg" alt="publication">
                </div>
              </div>
              
           </div>
         
       </div>

 <div style="display: flex; justify-content: space-between; align-items: center;">
  <h2>Patents and Software Copyrights</h2>
 </div>
 
      <div class="mb-3">
        
        <div class="article-list spacing ">
          <div class="publication-text">
            <p>
            
              <b>一种三自由度机械臂自动鼠von Frey测痛仪</b>
              
              <!-- 显示所有的作者 -->
              
                <br>
              <span class="authors">
              <!-- 添加一个变量 -->

                
                  
                  
                    
                  
                  
                     
                  
                  周晗,  
                
                  
                  
                    
                  
                  
                     
                  
                  叶楠昕,  
                
                  
                  
                    
                  
                  
                     
                  
                  刘昱辰,  
                
                  
                  
                    
                  
                  
                     
                  
                  胡思昊,  
                
                  
                  
                    
                  
                  
                     
                  
                  曾喻江,  
                
                  
                  
                    
                  
                  
                  裴磊 
                
            
            </span>
  
              
              
                                , Nov. 2025
                    
                    , Patent No.: ZL 2024 2 2823718.2
                  
              </p>
                
  
  
            </div>
              
           </div>
         
       </div>





            <!-- </div> -->
            <div class="row mt-2">
    
    <p id="copyright">
        
        Unless otherwise stated, all content on this blog is licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a><span class="d-none d-sm-inline"> | </span>
        <a href="/atom.xml" target="_blank" rel="noopener" class="btn btn-outline-secondary btn-sm">RSS subscribe</a> <span class="d-none d-sm-inline"> | </span>
        <a href="/friends">Friends</a>
        <br>
        
        <a target="_blank" rel="noopener" href="https://beian.miit.gov.cn">鄂ICP备2025091178号</a> <span class="d-none d-sm-inline"> | </span>
        <span class="d-none d-sm-inline"> 
        


        


    
        
            对应的中文页面尚未上线
        
    

    </p>

    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=250&t=n&d=tomGCTh1CxWc_-uuNllmA6Nl5mVnvkzy2svb86OrEyc&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
    
    
</div> 
          </div>
    
        </div>
    
      </div>
  
<script src="/js/main.js"></script>

</body>

</html>